{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Классификация текстов при помощи сверточных сетей\n",
    "\n",
    "__Автор__: Никита Владимирович Блохин (NVBlokhin@fa.ru)\n",
    "\n",
    "Финансовый университет, 2020 г."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "phEw721T9SYW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[3, 12, 6, 21, 1, 18, 9, 19]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "alphabet = list(set(nltk.word_tokenize(text.replace(\".\", \"\"))))\n",
    "word2index = {w: i for i, w in enumerate(alphabet)}\n",
    "first_sentence = nltk.sent_tokenize(text)[0].replace(\".\", \"\")\n",
    "[word2index[w] for w in nltk.word_tokenize(first_sentence)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RZS4XLV0-buf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 1., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "alphabet = list(set(nltk.word_tokenize(text.replace(\".\", \"\"))))\n",
    "word2index = {w: i for i, w in enumerate(alphabet)}\n",
    "first_sentence = nltk.sent_tokenize(text)[0].replace(\".\", \"\")\n",
    "words = nltk.word_tokenize(first_sentence)\n",
    "vectors = torch.zeros(len(words), len(alphabet))\n",
    "indices = [(i, word2index[w]) for i, w in enumerate(words)]\n",
    "vectors[list(zip(*indices))] = 1\n",
    "vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-4.9968e-01, -1.0670e+00,  1.1149e+00, -1.4067e-01,  8.0575e-01,\n         -9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n         -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n         -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n          7.1009e-01,  1.6459e+00, -1.3602e+00,  3.4457e-01,  5.1987e-01],\n        [ 1.4628e+00, -6.2043e-01,  9.8839e-01, -4.3218e-01, -6.2322e-01,\n         -2.1625e-01, -4.8868e-01,  7.8696e-01,  1.0759e-01, -1.0715e+00,\n         -1.1665e-01, -1.0170e+00, -1.1980e+00,  4.7844e-01, -1.2295e+00,\n         -1.3700e+00,  1.5435e+00, -3.3207e-02, -4.1863e-01, -2.5560e-01,\n         -1.2923e-01, -5.4595e-02,  4.0835e-01,  1.1264e+00,  1.9351e+00],\n        [-2.0106e-01, -1.1793e-01,  1.9220e-01, -7.7216e-01, -1.9003e+00,\n          1.3068e-01, -7.0429e-01,  3.1472e-01,  1.5739e-01,  3.8536e-01,\n          9.6715e-01, -9.9108e-01,  3.0161e-01, -1.0732e-01,  9.9846e-01,\n         -4.9871e-01,  7.6111e-01,  6.1830e-01,  3.1405e-01,  2.1333e-01,\n         -1.2005e-01,  3.6046e-01, -3.1403e-01, -1.0787e+00,  2.4081e-01],\n        [ 1.2532e+00, -4.4452e-01,  8.1845e-01, -8.1802e-01,  3.6032e-01,\n         -1.6146e+00, -2.4734e+00,  3.6156e-02, -3.4222e-01, -3.8169e-01,\n         -5.6879e-02,  8.4362e-01,  6.8287e-01,  3.3944e+00, -1.6688e+00,\n          5.1086e-01, -2.8599e-01,  3.3505e-01,  1.1719e+00,  1.2955e+00,\n          8.9086e-01, -4.8985e-01, -1.1727e+00, -6.8705e-01, -2.3349e+00],\n        [-5.8550e-01, -1.7340e-01,  1.8348e-01,  1.3894e+00,  1.5863e+00,\n          9.4630e-01, -8.4368e-01, -6.1358e-01,  3.1593e-02, -4.9268e-01,\n          2.4841e-01,  4.3970e-01,  1.1241e-01,  6.4079e-01,  4.4116e-01,\n         -1.0231e-01,  7.9244e-01, -2.8967e-01,  5.2507e-02,  5.2286e-01,\n          2.3022e+00, -1.4689e+00, -1.5867e+00, -6.7309e-01,  8.7283e-01],\n        [ 1.8024e+00, -1.0597e+00,  3.4028e+00, -5.6867e-01, -4.7549e-01,\n          1.7432e+00, -2.0441e-01, -3.1641e-01,  1.2937e+00,  1.3453e+00,\n          1.9394e-01,  1.5717e+00, -3.8274e-01,  1.3951e+00,  3.4275e-01,\n         -1.6045e+00, -5.8731e-01,  6.0039e-01,  4.3780e-01, -9.6455e-02,\n          3.3027e-01, -1.8752e-01, -1.4271e+00,  5.9255e-01, -1.1582e+00],\n        [ 1.5713e+00,  1.9161e-01,  3.7994e-01, -1.4476e-01,  6.3762e-01,\n         -2.8129e-01, -1.3299e+00, -1.4201e-01, -5.3415e-01, -5.2338e-01,\n          8.6150e-01, -8.8696e-01,  8.3877e-01,  1.1529e+00, -1.7611e+00,\n         -1.4777e+00, -1.7557e+00,  7.6166e-02, -1.0786e+00,  1.4403e+00,\n         -1.1059e-01,  5.7686e-01, -1.6917e-01, -6.4025e-02,  1.0384e+00],\n        [ 3.5761e-02,  2.1601e-01, -9.1608e-01,  1.5599e+00, -3.1537e+00,\n         -5.6110e-01, -4.3030e-01, -3.3323e-01, -1.5464e+00, -1.4717e-02,\n          1.2251e+00,  1.5936e+00, -1.6315e+00, -5.6877e-02,  6.2966e-01,\n          2.7117e-01, -6.8598e-01, -1.0918e+00,  1.6797e+00, -8.8082e-01,\n          5.8003e-01,  3.6423e-01,  8.8134e-02, -1.3069e+00, -7.0637e-01]],\n       grad_fn=<EmbeddingBackward0>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "embeds = nn.Embedding(num_embeddings=len(alphabet), embedding_dim=len(alphabet))\n",
    "indices = torch.tensor([word2index[w] for w in nltk.word_tokenize(first_sentence)])\n",
    "embeds(indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1 Считать файл `surnames/surnames.csv`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French\n2      Kore     English\n3     Koury      Arabic\n4    Lebzak     Russian",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_df = pd.read_csv(DATA_DIR / \"surnames.csv\")\n",
    "surnames_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2 Закодировать национальности числами, начиная с 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "    surname nationality  target\n0  Woodford     English       4\n1      Coté      French       5\n2      Kore     English       4\n3     Koury      Arabic       0\n4    Lebzak     Russian      14",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kore</td>\n      <td>English</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Koury</td>\n      <td>Arabic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lebzak</td>\n      <td>Russian</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_labeler = LabelEncoder()\n",
    "surnames_df[\"target\"] = surnames_labeler.fit_transform(surnames_df[\"nationality\"])\n",
    "print(f\"classes: {len(surnames_labeler.classes_)}\")\n",
    "surnames_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    pad = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, series: pd.Series):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for w in map(str.lower, series):\n",
    "            uniques.update(w)\n",
    "            max_len = max(len(w), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, *uniques]\n",
    "        self.max_len = max_len\n",
    "        self.ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in word]\n",
    "        # дополняем до одинакового размера индексом служебного символа\n",
    "        indices += [self.ch2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.pad], as_tuple=True)[0]  # noqa\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]  # отрезаем служебные символы\n",
    "        return \"\".join(self.alphabet[i] for i in indices)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30, 40, 29,  5, 45,  2, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]) kovalev\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(surnames_df[\"surname\"])\n",
    "indices = vocab.encode(\"kovalev\")\n",
    "print(indices, vocab.decode(indices))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса>\n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, vocab: Vocab, transform: t.Callable = None):\n",
    "        self.surnames = df[\"surname\"].tolist()\n",
    "\n",
    "        if transform:\n",
    "            # 1 раз transform - прохождение эпох быстрее\n",
    "            size = transform(self.surnames[0]).size()\n",
    "            self.data = torch.vstack([transform(w) for w in self.surnames]).view(len(self.surnames), *size)\n",
    "        else:\n",
    "            self.data = self.surnames\n",
    "        self.targets = torch.tensor(df[\"target\"], dtype=torch.long)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def to_indices(word: str) -> torch.Tensor:\n",
    "    return vocab.encode(word.lower())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "((tensor([37, 40, 40, 22, 44, 40, 50, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n  tensor(4)),\n (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.],\n          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0.]]),\n  tensor(4)))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(word: str) -> torch.Tensor:\n",
    "    vectors = torch.zeros(vocab.max_len, len(vocab.alphabet))\n",
    "    indices = [(i, vocab.ch2i[ch]) for i, ch in enumerate(word.lower())]\n",
    "    vectors[list(zip(*indices))] = 1\n",
    "    return vectors\n",
    "surnames_indices_dataset = SurnamesDataset(surnames_df, vocab, transform=to_indices)\n",
    "surnames_one_hot_dataset = SurnamesDataset(surnames_df, vocab, transform=one_hot)\n",
    "surnames_indices_dataset[0], surnames_one_hot_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3 Разбить датасет на обучающую и тестовую выборку"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784 2196\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_indices_dataset, test_indices_dataset = train_test_split(surnames_indices_dataset, train_part=0.8)\n",
    "train_one_hot_dataset, test_one_hot_dataset = train_test_split(surnames_one_hot_dataset, train_part=0.8)\n",
    "print(len(train_indices_dataset), len(test_indices_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6. Обучить классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта:\n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: Vocab,\n",
    "            out_features: int,\n",
    "            embedding_dim: int = 128,\n",
    "            use_embedding: bool = True,\n",
    "            debug: bool = False,\n",
    "    ):\n",
    "        super(SurnamesClassifier, self).__init__()\n",
    "        self.use_embedding = use_embedding\n",
    "        self.debug = debug\n",
    "        self.embedding_dim = embedding_dim\n",
    "        last_conv_out_channels = 64\n",
    "        adaptive_avg_pool = 8\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab.alphabet), embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=last_conv_out_channels, kernel_size=3),\n",
    "            nn.BatchNorm1d(num_features=last_conv_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(adaptive_avg_pool)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_conv_out_channels * adaptive_avg_pool, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, out_features),\n",
    "        )\n",
    "        if self.debug:\n",
    "            self.forward = self._debug_forward\n",
    "        else:\n",
    "            self.forward = self._forward\n",
    "\n",
    "    def _forward(self, x: torch.Tensor):\n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "            x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "    def _debug_forward(self, x: torch.Tensor):\n",
    "        print(\"x: \", x.size())\n",
    "        if self.use_embedding:\n",
    "            x = self.embedding(x)\n",
    "            print(\"embedding: \", x.size())\n",
    "        else:\n",
    "            x = F.pad(x, (0, self.embedding_dim - x.size(2), 0, 0), value=0)\n",
    "            print(\"pad: \", x.size())\n",
    "\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        print(\"reshape: \", x.size())\n",
    "        x = self.features(x)\n",
    "        print(\"features: \", x.size())\n",
    "        x = self.avgpool(x)\n",
    "        print(\"avgpool: \", x.size())\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(\"flatten: \", x.size())\n",
    "        x = self.classifier(x)\n",
    "        print(\"classifier: \", x.size())\n",
    "        return torch.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "common_net = SurnamesClassifier(vocab, len(surnames_labeler.classes_)).to(DEVICE)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(common_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.220617  [    0/ 8784]\n",
      "loss: 1.715759  [ 4000/ 8784]\n",
      "loss: 1.885436  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.555556, Avg loss: 1.595021 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.346738  [    0/ 8784]\n",
      "loss: 1.754438  [ 4000/ 8784]\n",
      "loss: 1.677387  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.599271, Avg loss: 1.429762 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.331221  [    0/ 8784]\n",
      "loss: 0.649125  [ 4000/ 8784]\n",
      "loss: 1.476949  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.609290, Avg loss: 1.350857 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.286790  [    0/ 8784]\n",
      "loss: 1.122464  [ 4000/ 8784]\n",
      "loss: 0.976356  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.628871, Avg loss: 1.296634 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.333323  [    0/ 8784]\n",
      "loss: 0.648875  [ 4000/ 8784]\n",
      "loss: 1.170995  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.642077, Avg loss: 1.290461 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.020304  [    0/ 8784]\n",
      "loss: 0.926924  [ 4000/ 8784]\n",
      "loss: 0.573694  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.641621, Avg loss: 1.298861 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.255714  [    0/ 8784]\n",
      "loss: 1.660677  [ 4000/ 8784]\n",
      "loss: 0.244331  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.650273, Avg loss: 1.273809 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.399933  [    0/ 8784]\n",
      "loss: 2.052178  [ 4000/ 8784]\n",
      "loss: 2.006327  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.646630, Avg loss: 1.280940 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.689376  [    0/ 8784]\n",
      "loss: 1.387198  [ 4000/ 8784]\n",
      "loss: 1.562039  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.649818, Avg loss: 1.327856 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.855647  [    0/ 8784]\n",
      "loss: 1.723984  [ 4000/ 8784]\n",
      "loss: 1.370439  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.651639, Avg loss: 1.283679 \n",
      "\n",
      "CPU times: total: 5min 25s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "common_net.use_embedding = False\n",
    "_ = common_train(\n",
    "    epochs=10,\n",
    "    model=common_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=DataLoader(train_one_hot_dataset, batch_size=8, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_one_hot_dataset, batch_size=512),\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "embeddings_net = SurnamesClassifier(vocab, len(surnames_labeler.classes_)).to(DEVICE)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(embeddings_net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.864955  [    0/ 8784]\n",
      "loss: 1.526119  [ 4000/ 8784]\n",
      "loss: 1.139495  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.555100, Avg loss: 1.574398 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.502816  [    0/ 8784]\n",
      "loss: 2.243810  [ 4000/ 8784]\n",
      "loss: 0.973268  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.637978, Avg loss: 1.331169 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 2.118947  [    0/ 8784]\n",
      "loss: 1.777347  [ 4000/ 8784]\n",
      "loss: 0.757749  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.654827, Avg loss: 1.219785 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.370441  [    0/ 8784]\n",
      "loss: 1.380106  [ 4000/ 8784]\n",
      "loss: 1.284697  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.683971, Avg loss: 1.130855 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.645624  [    0/ 8784]\n",
      "loss: 1.600794  [ 4000/ 8784]\n",
      "loss: 0.607627  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.684882, Avg loss: 1.115376 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.769589  [    0/ 8784]\n",
      "loss: 1.099187  [ 4000/ 8784]\n",
      "loss: 0.378930  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.693078, Avg loss: 1.064560 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.663228  [    0/ 8784]\n",
      "loss: 1.310467  [ 4000/ 8784]\n",
      "loss: 0.188149  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.699909, Avg loss: 1.146194 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.333110  [    0/ 8784]\n",
      "loss: 1.280218  [ 4000/ 8784]\n",
      "loss: 0.261357  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.704007, Avg loss: 1.143740 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 1.115383  [    0/ 8784]\n",
      "loss: 1.328889  [ 4000/ 8784]\n",
      "loss: 0.980664  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.713115, Avg loss: 1.071422 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.273081  [    0/ 8784]\n",
      "loss: 0.271664  [ 4000/ 8784]\n",
      "loss: 0.667502  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.707650, Avg loss: 1.208268 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.278220  [    0/ 8784]\n",
      "loss: 0.200003  [ 4000/ 8784]\n",
      "loss: 0.423023  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.712659, Avg loss: 1.207909 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.248427  [    0/ 8784]\n",
      "loss: 0.092666  [ 4000/ 8784]\n",
      "loss: 2.049445  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.709472, Avg loss: 1.296231 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.368756  [    0/ 8784]\n",
      "loss: 0.151465  [ 4000/ 8784]\n",
      "loss: 1.342989  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.714481, Avg loss: 1.241643 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.308070  [    0/ 8784]\n",
      "loss: 0.394839  [ 4000/ 8784]\n",
      "loss: 0.303843  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.708106, Avg loss: 1.313883 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.280828  [    0/ 8784]\n",
      "loss: 1.295854  [ 4000/ 8784]\n",
      "loss: 1.196941  [ 8000/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.702186, Avg loss: 1.343712 \n",
      "\n",
      "CPU times: total: 9min 42s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_net.use_embedding = True\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=embeddings_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=DataLoader(train_indices_dataset, batch_size=8, shuffle=True),\n",
    "    test_dataloader=DataLoader(test_indices_dataset, batch_size=512),\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.702186, Avg loss: 1.343712 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(\n",
    "    dataloader=DataLoader(test_indices_dataset, batch_size=512),\n",
    "    model=embeddings_net,\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def inference(\n",
    "        surname: str,\n",
    "        target: str,\n",
    "        model: nn.Module,\n",
    "        vocab: Vocab,\n",
    "        labeler: LabelEncoder,\n",
    "        k: int = 3,\n",
    "        device: str = \"cpu\",\n",
    "):\n",
    "    x = vocab.encode(surname.lower())\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    pred_proba, pred_label_indices = F.softmax(pred, 1).topk(k, dim=1)\n",
    "    pred_labels = labeler.inverse_transform(pred_label_indices.squeeze().cpu())\n",
    "\n",
    "    predicts = \", \".join(\n",
    "        [f\"{label} ({prob:.2f})\" for (label, prob) in zip(pred_labels, pred_proba.squeeze())]\n",
    "    )\n",
    "    print(f\"Surname : {surname}\")\n",
    "    print(f\"True    : {target}\")\n",
    "    print(f\"Predicts: {predicts}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GHjCRqQg1sw5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "students = [\n",
    "    \"Woodford\",\n",
    "    \"Kore\",\n",
    "    \"Essop\",\n",
    "    \"Jefferson\",\n",
    "    \"Dorrington\",\n",
    "    \"Jeffries\",\n",
    "    \"Douthwaite\",\n",
    "    \"Readle\",\n",
    "    \"Jones\",\n",
    "    \"Topham\",\n",
    "    \"Bellamy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname : Woodford\n",
      "True    : English\n",
      "Predicts: English (1.00), Scottish (0.00), German (0.00)\n",
      "\n",
      "Surname : Kore\n",
      "True    : English\n",
      "Predicts: Czech (0.69), Polish (0.15), English (0.07)\n",
      "\n",
      "Surname : Essop\n",
      "True    : English\n",
      "Predicts: English (0.98), Russian (0.01), French (0.01)\n",
      "\n",
      "Surname : Jefferson\n",
      "True    : English\n",
      "Predicts: English (1.00), Scottish (0.00), German (0.00)\n",
      "\n",
      "Surname : Dorrington\n",
      "True    : English\n",
      "Predicts: English (0.99), German (0.00), Scottish (0.00)\n",
      "\n",
      "Surname : Jeffries\n",
      "True    : English\n",
      "Predicts: English (0.99), French (0.00), Scottish (0.00)\n",
      "\n",
      "Surname : Douthwaite\n",
      "True    : English\n",
      "Predicts: English (1.00), German (0.00), Scottish (0.00)\n",
      "\n",
      "Surname : Readle\n",
      "True    : English\n",
      "Predicts: English (0.98), Irish (0.01), Scottish (0.00)\n",
      "\n",
      "Surname : Jones\n",
      "True    : English\n",
      "Predicts: English (0.93), Scottish (0.03), German (0.02)\n",
      "\n",
      "Surname : Topham\n",
      "True    : English\n",
      "Predicts: English (0.84), German (0.16), French (0.00)\n",
      "\n",
      "Surname : Bellamy\n",
      "True    : English\n",
      "Predicts: English (0.99), Irish (0.00), Scottish (0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for surname in students:\n",
    "    inference(\n",
    "        surname=surname,\n",
    "        target=\"English\",\n",
    "        model=embeddings_net,\n",
    "        vocab=vocab,\n",
    "        labeler=surnames_labeler,\n",
    "        device=DEVICE,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Вывод:** использование Embedding позволило увеличить точность модели."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def get_pos(word: str) -> str:\n",
    "    tag = nltk.pos_tag([word])[0][1]\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_review(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # удаляем все символы кроме букв латинского алфавита\n",
    "    text = re.sub(r\"[^a-z]\", repl=\" \", string=text, flags=re.MULTILINE)\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    words = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word not in STOPWORDS:  # удаляем стоп-слова до лемматизации - так можно чуть-чуть сэкономить\n",
    "            lemma = lemmatizer.lemmatize(word, pos=get_pos(word))\n",
    "            # удаляем стоп-слова, наивное предположение - не брать леммы короче 3-х символов дало значительный прирост точности\n",
    "            if lemma not in STOPWORDS and len(lemma) > 2:\n",
    "                words.append(lemma)\n",
    "\n",
    "    return \" \".join(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, positive_path: Path, negative_path: Path, seed: int = None):\n",
    "        self.positive_path = positive_path\n",
    "        self.negative_path = negative_path\n",
    "        self.positive_reviews = self.read_reviews(positive_path, preprocess_review)\n",
    "        self.negative_reviews = self.read_reviews(negative_path, preprocess_review)\n",
    "\n",
    "        data = self.positive_reviews + self.negative_reviews\n",
    "        targets = torch.cat([torch.ones(len(self.positive_reviews)), torch.zeros(len(self.negative_reviews))])\n",
    "\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        indices = torch.randperm(len(data))\n",
    "\n",
    "        self.data = [data[i] for i in indices]\n",
    "        self.targets = targets[indices].to(torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_reviews(path: Path, process: t.Callable[[str], str]) -> list[str]:\n",
    "        reviews = []\n",
    "        with open(path) as f:\n",
    "            for review in f.readlines():\n",
    "                review = process(review)\n",
    "                if review:\n",
    "                    reviews.append(review)\n",
    "        return reviews\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(10660,\n ('none happily ever spangle monsoon wedding late marriage part make dover kosashvili outstanding feature debut potent',\n  tensor(0)))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_dataset = ReviewsDataset(\n",
    "    DATA_DIR / \"positive_reviews.txt\",\n",
    "    DATA_DIR / \"negative_reviews.txt\",\n",
    "    seed=0,\n",
    ")\n",
    "len(reviews_dataset), reviews_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(8528, 2132)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_reviews, test_reviews = train_test_split(reviews_dataset, train_part=0.8)\n",
    "len(train_reviews), len(test_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class ReviewsVocab:\n",
    "    pad = \"<PAD>\"\n",
    "    unknown = \"<UNK>\"\n",
    "\n",
    "    def __init__(self, reviews: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for review in reviews:\n",
    "            words = nltk.word_tokenize(review)\n",
    "            uniques.update(words)\n",
    "            max_len = max(len(words), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, self.unknown, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        w2i = {w: i for i, w in enumerate(self.alphabet)}\n",
    "        # если ключ отсутствует, будет возвращена 1 - индекс служебного символа\n",
    "        self.w2i = defaultdict(lambda: 1, w2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    @lru_cache(maxsize=8192)  # сомнительная эффективность? Ну да\n",
    "    def encode(self, review: str) -> torch.Tensor:\n",
    "        indices = [self.w2i[w] for w in nltk.word_tokenize(review)]\n",
    "        indices += [self.w2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.w2i[self.pad], as_tuple=True)[0]  # noqa\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \" \".join(self.alphabet[i] for i in indices)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet: 13287 longest: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([    1,     1,     1,  6231, 11318,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0]),\n '<UNK> <UNK> <UNK> neutral review')"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ReviewsVocab([review for review, _ in train_reviews])\n",
    "print(f\"alphabet: {len(vocab)}\", f\"longest: {vocab.max_len}\")\n",
    "encoded = vocab.encode(\"this is a neutral review\")\n",
    "encoded, vocab.decode(encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2. Обучите классификатор.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`\n",
    "    - подберите адекватную размерность вектора эмбеддинга:\n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class ReviewsClassifier(nn.Module):\n",
    "    LAST_CONV_OUT_CHANNELS = 64\n",
    "    ADAPTIVE_AVG_POOL = 8\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super(ReviewsClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=self.LAST_CONV_OUT_CHANNELS, kernel_size=2),\n",
    "            nn.BatchNorm1d(num_features=self.LAST_CONV_OUT_CHANNELS),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(self.ADAPTIVE_AVG_POOL)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.LAST_CONV_OUT_CHANNELS * self.ADAPTIVE_AVG_POOL, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape(x.size(0), x.size(2), x.size(1))\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def collate(batch: t.List[t.Tuple[str, torch.Tensor]]) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for x, y in batch:\n",
    "        xs.append(vocab.encode(x))\n",
    "        ys.append(y)\n",
    "    return torch.vstack(xs), torch.hstack(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "net = ReviewsClassifier(num_embeddings=len(vocab), embedding_dim=128).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.000914092001)  # а почему нет?\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode=\"min\",\n",
    "    patience=5,\n",
    "    factor=0.333333,\n",
    "    min_lr=0.000001,\n",
    "    threshold=0.001,\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_reviews, batch_size=22, collate_fn=collate, shuffle=True)\n",
    "test_dataloader = DataLoader(test_reviews, batch_size=512, collate_fn=collate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 0.708673  [    0/ 8528]\n",
      "loss: 0.725537  [ 3300/ 8528]\n",
      "loss: 0.686096  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.497655, Avg loss: 0.693041 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 0.689131  [    0/ 8528]\n",
      "loss: 0.691586  [ 3300/ 8528]\n",
      "loss: 0.699118  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.502345, Avg loss: 0.692707 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.695276  [    0/ 8528]\n",
      "loss: 0.670025  [ 3300/ 8528]\n",
      "loss: 0.692704  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.508443, Avg loss: 0.691847 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.722572  [    0/ 8528]\n",
      "loss: 0.680059  [ 3300/ 8528]\n",
      "loss: 0.700981  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.600844, Avg loss: 0.678472 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.716668  [    0/ 8528]\n",
      "loss: 0.589112  [ 3300/ 8528]\n",
      "loss: 0.600119  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.629456, Avg loss: 0.630850 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.587483  [    0/ 8528]\n",
      "loss: 0.684042  [ 3300/ 8528]\n",
      "loss: 0.649378  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.661820, Avg loss: 0.642815 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.324635  [    0/ 8528]\n",
      "loss: 0.485217  [ 3300/ 8528]\n",
      "loss: 0.360737  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.662289, Avg loss: 0.710335 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.119740  [    0/ 8528]\n",
      "loss: 0.380485  [ 3300/ 8528]\n",
      "loss: 0.185781  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.661351, Avg loss: 0.951132 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.463050  [    0/ 8528]\n",
      "loss: 0.073718  [ 3300/ 8528]\n",
      "loss: 0.150167  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.667917, Avg loss: 1.060270 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.073269  [    0/ 8528]\n",
      "loss: 0.048044  [ 3300/ 8528]\n",
      "loss: 0.304059  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.661351, Avg loss: 1.320000 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.227068  [    0/ 8528]\n",
      "loss: 0.044487  [ 3300/ 8528]\n",
      "loss: 0.195796  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.670263, Avg loss: 1.556174 \n",
      "\n",
      "Epoch 00011: reducing learning rate of group 0 to 3.0470e-04.\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.058229  [    0/ 8528]\n",
      "loss: 0.014033  [ 3300/ 8528]\n",
      "loss: 0.013933  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.672608, Avg loss: 1.782800 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.003541  [    0/ 8528]\n",
      "loss: 0.045519  [ 3300/ 8528]\n",
      "loss: 0.001094  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.668386, Avg loss: 1.914093 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.008323  [    0/ 8528]\n",
      "loss: 0.005191  [ 3300/ 8528]\n",
      "loss: 0.014317  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.674953, Avg loss: 1.923777 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.044805  [    0/ 8528]\n",
      "loss: 0.000514  [ 3300/ 8528]\n",
      "loss: 0.011787  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.673546, Avg loss: 2.168956 \n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.001597  [    0/ 8528]\n",
      "loss: 0.001352  [ 3300/ 8528]\n",
      "loss: 0.033026  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.676360, Avg loss: 2.222012 \n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.000231  [    0/ 8528]\n",
      "loss: 0.033405  [ 3300/ 8528]\n",
      "loss: 0.013974  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.674015, Avg loss: 2.284595 \n",
      "\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0157e-04.\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.006737  [    0/ 8528]\n",
      "loss: 0.015911  [ 3300/ 8528]\n",
      "loss: 0.008131  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.673077, Avg loss: 2.400737 \n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 0.000552  [    0/ 8528]\n",
      "loss: 0.000728  [ 3300/ 8528]\n",
      "loss: 0.000819  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.674953, Avg loss: 2.444745 \n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.008466  [    0/ 8528]\n",
      "loss: 0.007904  [ 3300/ 8528]\n",
      "loss: 0.033260  [ 6600/ 8528]\n",
      "Test Error: \n",
      " Accuracy: 0.674953, Avg loss: 2.439811 \n",
      "\n",
      "CPU times: total: 15min 45s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = common_train(\n",
    "    epochs=20,\n",
    "    model=net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    verbose=150,\n",
    "    device=DEVICE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "У модели явная проблема с переобучением. После 3-ей эпохи тестовая ошибка начала увеличиваться,\n",
    "в то же время ошибка на обучающей выборки быстро приблизилась к 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.69      0.68      1061\n",
      "    positive       0.68      0.66      0.67      1071\n",
      "\n",
      "    accuracy                           0.67      2132\n",
      "   macro avg       0.68      0.68      0.67      2132\n",
      "weighted avg       0.68      0.67      0.67      2132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(net, test_dataloader, DEVICE)\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[\"negative\", \"positive\"],\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def inference(\n",
    "        review: str,\n",
    "        target: str,\n",
    "        model: nn.Module,\n",
    "        vocab: ReviewsVocab,\n",
    "        target_names: list[str],\n",
    "        device: str = \"cpu\",\n",
    "):\n",
    "    x = vocab.encode(preprocess_review(review))\n",
    "    x = x.to(device)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    pred_proba, pred_label_idx = F.softmax(pred, 1).max(dim=1)\n",
    "    pred_label = target_names[pred_label_idx.cpu()]\n",
    "\n",
    "    print(f\"Review : {review}\")\n",
    "    print(f\"True   : {target}\")\n",
    "    print(f\"Predict: {pred_label} ({pred_proba.item():.2f})\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review : No intrigue, poor character disclosure.\n",
      "True   : negative\n",
      "Predict: positive (1.00)\n",
      "\n",
      "Review : A fascinating story. The actors played their characters perfectly.\n",
      "True   : positive\n",
      "Predict: negative (1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    (\"No intrigue, poor character disclosure.\", \"negative\"),\n",
    "    (\"A fascinating story. The actors played their characters perfectly.\", \"positive\"),\n",
    "]\n",
    "for review, target in reviews:\n",
    "    inference(\n",
    "        review=review,\n",
    "        target=target,\n",
    "        model=net,\n",
    "        vocab=vocab,\n",
    "        target_names=[\"negative\", \"positive\"],\n",
    "        device=DEVICE,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPUQGO96Qffh8FYHODzJzxf",
   "collapsed_sections": [],
   "name": "blank__06_CNN_embeddings_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}