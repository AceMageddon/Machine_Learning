{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1620559384951,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "0qOQwNlZbFiO",
    "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU device\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data/\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE.upper()} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def on_cuda(device: str) -> bool:\n",
    "    return device == \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        verbose: int = 100,\n",
    "        on_epoch_end: t.Callable[[], None] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.List[float]:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        if test_dataloader:\n",
    "            test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "\n",
    "        if on_epoch_end:\n",
    "            on_epoch_end()\n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss, num_batches = 0, len(dataloader)\n",
    "    correct, total = 0, 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "\n",
    "        y_test = torch.flatten(y)\n",
    "        y_pred = torch.flatten(pred.argmax(1))\n",
    "        total += y_test.size(0)\n",
    "        correct += (y_pred == y_test).sum()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Error: \\n\"\n",
    "          f\"\\tAccuracy: {accuracy:>4f}, Loss: {avg_loss:>8f}\")\n",
    "\n",
    "    return avg_loss, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.flatten(torch.vstack(y_test).detach().cpu()), torch.flatten(torch.vstack(y_pred).detach().cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmWCBWxrBUB3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Генерирование русских имен при помощи RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
    "\n",
    "1.1 На основе файла name_rus.txt создайте датасет.\n",
    "  * Учтите, что имена могут иметь различную длину\n",
    "  * Добавьте 4 специальных токена:\n",
    "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
    "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
    "    * `<SOS>` для обозначения начала последовательности;\n",
    "    * `<EOS>` для обозначения конца последовательности.\n",
    "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
    "    * в начало последовательности добавьте токен `<SOS>`;\n",
    "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
    "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
    "\n",
    "  Пример:\n",
    "  ```\n",
    "  s = 'The cat sat on the mat'\n",
    "  # преобразуем в индексы\n",
    "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  # получаем x и y (__getitem__)\n",
    "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
    "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Будем предсказывать каждую следующую букву в имени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NamesVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "    SOS = \"<SOS>\"\n",
    "    SOS_IDX = 2\n",
    "    EOS = \"<EOS>\"\n",
    "    EOS_IDX = 3\n",
    "\n",
    "    def __init__(self, names: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for name in map(str.lower, names):\n",
    "            uniques.update(name)\n",
    "            max_len = max(len(name), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, self.SOS, self.EOS, *uniques]\n",
    "        self.max_len = max_len + 2  # место для <SOS> и <EOS>\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        name = [*name, self.EOS]\n",
    "        if not shift:\n",
    "            name = [self.SOS, *name]\n",
    "        indices = [self.ch2i[ch] for ch in name]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class NamesDataset:\n",
    "    names: t.List[str]\n",
    "    vocab: NamesVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        self.names = self.read_names(path)\n",
    "        self.vocab = NamesVocab(self.names)\n",
    "\n",
    "        self.data = torch.vstack([self.encode(name, shift=False) for name in self.names])\n",
    "        self.targets = torch.vstack([self.encode(name, shift=True) for name in self.names])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_names(path: Path) -> t.List[str]:\n",
    "        with open(path, encoding=\"cp1251\") as f:\n",
    "            return list(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "    def encode(self, name: str, shift: bool = False) -> torch.Tensor:\n",
    "        return self.vocab.encode(name, shift=shift)\n",
    "\n",
    "    def decode(self, vector: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Такой метод кодирования позволяет сохранить на одну букву больше, чем предложенный в задании - теряем `<SOS>`, но сохраняем первый и последний символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1988\n"
     ]
    },
    {
     "data": {
      "text/plain": "('авдокея',\n tensor([ 2,  9, 19, 10, 33, 12,  6, 25,  3,  0,  0,  0,  0,  0,  0]),\n tensor([ 9, 19, 10, 33, 12,  6, 25,  3,  0,  0,  0,  0,  0,  0,  0]))"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dataset = NamesDataset(DATA_DIR / \"name_rus.txt\")\n",
    "print(f\"n: {len(names_dataset)}\")\n",
    "(names_dataset.names[0], *names_dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590 398\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_names_dataset, test_names_dataset = train_test_split(names_dataset, train_part=0.8)\n",
    "print(len(train_names_dataset), len(test_names_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.2 Создайте и обучите модель для генерации фамилии.\n",
    "\n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
    "  * Используйте рекуррентные слои;\n",
    "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
    "  * Примерная схема реализации метода `forward`:\n",
    "  ```\n",
    "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
    "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
    "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
    "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
    "  ```\n",
    "\n",
    "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
    "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
    "  * Начальное скрытое состояние RNN `h_t = None`;\n",
    "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
    "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
    "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
    "\n",
    "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NamesRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            self.rnn_state = (state[0].detach(), state[1].detach())\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)\n",
    "        x = self.fc(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.reset_rnn_state()\n",
    "        return super().train(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def true_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    pred -= pred.min()\n",
    "    return pred / pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def softmax_prob(pred: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.softmax(pred, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def generate_name(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    "        prompt: str = None,\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    vocab = dataset.vocab\n",
    "    name_vec = [vocab.SOS_IDX]\n",
    "    if prompt:\n",
    "        name_vec += [vocab.ch2i[ch] for ch in prompt]\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(len(name_vec) - 1):\n",
    "        x = torch.tensor([[name_vec[i]]], device=device)\n",
    "        model(x)\n",
    "\n",
    "    for i in range(vocab.max_len - 2 - len(name_vec)):\n",
    "        x = torch.tensor([[name_vec[-1]]], device=device)\n",
    "        pred = model(x).squeeze()\n",
    "        if prob:\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:\n",
    "            next_ch_idx = pred.argmax()\n",
    "\n",
    "        if next_ch_idx == vocab.EOS_IDX:\n",
    "            break\n",
    "        name_vec.append(next_ch_idx.item())\n",
    "\n",
    "    return \"\".join(vocab.alphabet[i] for i in name_vec[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def on_epoch_end_generate_names(\n",
    "        model: NamesRNNGenerator,\n",
    "        dataset: NamesDataset,\n",
    ") -> t.Callable[[], None]:\n",
    "    def _on_epoch_end() -> None:\n",
    "        const = generate_name(model, dataset, device=DEVICE)\n",
    "        true_random = generate_name(model, dataset, prob=true_prob, device=DEVICE)\n",
    "        softmax_random = generate_name(model, dataset, prob=softmax_prob, device=DEVICE)\n",
    "        print(f\"\\tNames: {const} (max), {true_random} (prob), {softmax_random} (softmax)\")\n",
    "    return _on_epoch_end"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "names_gen_net = NamesRNNGenerator(\n",
    "    num_embeddings=len(names_dataset.vocab),\n",
    "    embedding_dim=8,\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.RNN,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(names_gen_net.parameters(), lr=0.001)\n",
    "train_dataloader = DataLoader(train_names_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_names_dataset, batch_size=128, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.516943  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.550694, Loss: 1.853318\n",
      "\tNames: а (max), яхнодпдвдшьн (prob), фшрлсл (softmax)\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 2.063714  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.619097, Loss: 1.331174\n",
      "\tNames: ла (max), етодэхмачркх (prob), кндчки (softmax)\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.282940  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.649653, Loss: 1.194422\n",
      "\tNames: ниннн (max), мвлштяые<PAD>йшн (prob), макун (softmax)\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.355991  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.672396, Loss: 1.126523\n",
      "\tNames: нита (max), нлнодкпинулю (prob), срсдяк (softmax)\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.083427  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.684201, Loss: 1.074423\n",
      "\tNames: леня (max), шресвлпюша (prob), кютр (softmax)\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 1.036385  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.693229, Loss: 1.033371\n",
      "\tNames: ниня (max), плфхы<PAD>сдипем (prob), латанда (softmax)\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.999216  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.695833, Loss: 1.005673\n",
      "\tNames: ниня (max), яеэврдуахпшб (prob), голор (softmax)\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 1.015202  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.695833, Loss: 0.986704\n",
      "\tNames: ниня (max), фкжшд<PAD>ыенюст (prob), линелишка (softmax)\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.907028  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.704687, Loss: 0.965373\n",
      "\tNames: леня (max), чтлмкаыавусб (prob), фейя (softmax)\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 1.063294  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.703299, Loss: 0.952401\n",
      "\tNames: ниня (max), лушкшян<PAD>уяфз (prob), алюна (softmax)\n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.984865  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.708681, Loss: 0.941408\n",
      "\tNames: леня (max), кн (prob), людьяка (softmax)\n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 1.073664  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.712674, Loss: 0.927600\n",
      "\tNames: миля (max), жкксдсхуй (prob), ликосна (softmax)\n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.921670  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.715972, Loss: 0.915122\n",
      "\tNames: леня (max), лосягхыур<PAD>ык (prob), гелури (softmax)\n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 1.061450  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.716319, Loss: 0.913361\n",
      "\tNames: леня (max), еюаонюырыхил (prob), вивеля (softmax)\n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.956012  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.719097, Loss: 0.902327\n",
      "\tNames: леня (max), врхьряюзеыда (prob), тонтан (softmax)\n",
      "\n",
      "Epoch 16\n",
      "--------------------------------\n",
      "loss: 0.922770  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.717535, Loss: 0.897820\n",
      "\tNames: леня (max), азцычсльтшо<PAD> (prob), яюха (softmax)\n",
      "\n",
      "Epoch 17\n",
      "--------------------------------\n",
      "loss: 0.946294  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.719792, Loss: 0.887705\n",
      "\tNames: нина (max), бспкю<SOS>йнятму (prob), вонюныч (softmax)\n",
      "\n",
      "Epoch 18\n",
      "--------------------------------\n",
      "loss: 0.977042  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.722569, Loss: 0.880169\n",
      "\tNames: леня (max), ацэсадыядьме (prob), нинюша (softmax)\n",
      "\n",
      "Epoch 19\n",
      "--------------------------------\n",
      "loss: 1.007974  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.720139, Loss: 0.876467\n",
      "\tNames: леня (max),  (prob), нидуша (softmax)\n",
      "\n",
      "Epoch 20\n",
      "--------------------------------\n",
      "loss: 0.931218  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.727431, Loss: 0.867471\n",
      "\tNames: леня (max), шсыипнывканб (prob), вер (softmax)\n",
      "\n",
      "Epoch 21\n",
      "--------------------------------\n",
      "loss: 0.924036  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.728819, Loss: 0.860404\n",
      "\tNames: настан (max), мбннвынежшды (prob), тинуха (softmax)\n",
      "\n",
      "Epoch 22\n",
      "--------------------------------\n",
      "loss: 0.878949  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.728472, Loss: 0.854738\n",
      "\tNames: лена (max), ола (prob), викила (softmax)\n",
      "\n",
      "Epoch 23\n",
      "--------------------------------\n",
      "loss: 0.803061  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.727951, Loss: 0.849906\n",
      "\tNames: лина (max), чроерзгунытх (prob), мируся (softmax)\n",
      "\n",
      "Epoch 24\n",
      "--------------------------------\n",
      "loss: 0.876921  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.732986, Loss: 0.844586\n",
      "\tNames: настан (max), ептики (prob), ликсетя (softmax)\n",
      "\n",
      "Epoch 25\n",
      "--------------------------------\n",
      "loss: 0.851977  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.734549, Loss: 0.848067\n",
      "\tNames: леня (max), яозбь (prob), тума (softmax)\n",
      "\n",
      "Epoch 26\n",
      "--------------------------------\n",
      "loss: 0.908516  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.733681, Loss: 0.834509\n",
      "\tNames: николина (max), кузбфь (prob), тася (softmax)\n",
      "\n",
      "Epoch 27\n",
      "--------------------------------\n",
      "loss: 0.900821  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.738194, Loss: 0.831803\n",
      "\tNames: леня (max), гврофудовсжа (prob), нелиона (softmax)\n",
      "\n",
      "Epoch 28\n",
      "--------------------------------\n",
      "loss: 0.893016  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.744965, Loss: 0.828253\n",
      "\tNames: вася (max), еваыдмшрены<PAD> (prob), алексан (softmax)\n",
      "\n",
      "Epoch 29\n",
      "--------------------------------\n",
      "loss: 0.854069  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.739410, Loss: 0.823052\n",
      "\tNames: лена (max), и (prob), вионника (softmax)\n",
      "\n",
      "Epoch 30\n",
      "--------------------------------\n",
      "loss: 0.851929  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.744271, Loss: 0.818504\n",
      "\tNames: леня (max), мус (prob), ганя (softmax)\n",
      "\n",
      "Epoch 31\n",
      "--------------------------------\n",
      "loss: 0.879045  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.750000, Loss: 0.811206\n",
      "\tNames: вася (max), ерпиьмдияныб (prob), нуханя (softmax)\n",
      "\n",
      "Epoch 32\n",
      "--------------------------------\n",
      "loss: 0.748728  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.749306, Loss: 0.807382\n",
      "\tNames: марина (max), ентдкуробмьт (prob), рира (softmax)\n",
      "\n",
      "Epoch 33\n",
      "--------------------------------\n",
      "loss: 0.822788  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.746354, Loss: 0.805577\n",
      "\tNames: марислав (max), яюб (prob), нелединка (softmax)\n",
      "\n",
      "Epoch 34\n",
      "--------------------------------\n",
      "loss: 0.803559  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.744271, Loss: 0.801734\n",
      "\tNames: лена (max), жэниьаяилбцс (prob), лилиинка (softmax)\n",
      "\n",
      "Epoch 35\n",
      "--------------------------------\n",
      "loss: 0.764701  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.748264, Loss: 0.799005\n",
      "\tNames: марислав (max), лгя (prob), рася (softmax)\n",
      "\n",
      "Epoch 36\n",
      "--------------------------------\n",
      "loss: 0.823772  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.746181, Loss: 0.796291\n",
      "\tNames: настана (max), м<UNK>янанархрэо (prob), менгина (softmax)\n",
      "\n",
      "Epoch 37\n",
      "--------------------------------\n",
      "loss: 0.839416  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.747917, Loss: 0.796226\n",
      "\tNames: митя (max), бзллкю<SOS>юбюкх (prob), хариама (softmax)\n",
      "\n",
      "Epoch 38\n",
      "--------------------------------\n",
      "loss: 0.764943  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.753299, Loss: 0.790029\n",
      "\tNames: митра (max), нненаяктг<SOS>ию (prob), маша (softmax)\n",
      "\n",
      "Epoch 39\n",
      "--------------------------------\n",
      "loss: 0.795254  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.754514, Loss: 0.791153\n",
      "\tNames: маринка (max), спсеурыовк (prob), юнта (softmax)\n",
      "\n",
      "Epoch 40\n",
      "--------------------------------\n",
      "loss: 0.779114  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.752951, Loss: 0.787939\n",
      "\tNames: марин (max), оудизенутфын (prob), реня (softmax)\n",
      "\n",
      "Epoch 41\n",
      "--------------------------------\n",
      "loss: 0.807111  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.753299, Loss: 0.783178\n",
      "\tNames: вася (max), эв<PAD>идавйухшр (prob), сеняша (softmax)\n",
      "\n",
      "Epoch 42\n",
      "--------------------------------\n",
      "loss: 0.731066  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.751042, Loss: 0.783757\n",
      "\tNames: миля (max), пдтеалаыанер (prob), никуша (softmax)\n",
      "\n",
      "Epoch 43\n",
      "--------------------------------\n",
      "loss: 0.779872  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.759375, Loss: 0.778452\n",
      "\tNames: марина (max), инкврмгчбк<SOS>ш (prob), катуня (softmax)\n",
      "\n",
      "Epoch 44\n",
      "--------------------------------\n",
      "loss: 0.810556  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758854, Loss: 0.775655\n",
      "\tNames: марина (max), гоягитычэзпл (prob), гинаня (softmax)\n",
      "\n",
      "Epoch 45\n",
      "--------------------------------\n",
      "loss: 0.741076  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.753472, Loss: 0.777453\n",
      "\tNames: миля (max), ишашлочцбопу (prob), заха (softmax)\n",
      "\n",
      "Epoch 46\n",
      "--------------------------------\n",
      "loss: 0.759956  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.755729, Loss: 0.774428\n",
      "\tNames: марина (max), ссчяа<SOS>шкркчт (prob), фелюня (softmax)\n",
      "\n",
      "Epoch 47\n",
      "--------------------------------\n",
      "loss: 0.774832  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.756424, Loss: 0.770298\n",
      "\tNames: марина (max), п<UNK>еоьясхантй (prob), людуха (softmax)\n",
      "\n",
      "Epoch 48\n",
      "--------------------------------\n",
      "loss: 0.727103  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.759549, Loss: 0.771030\n",
      "\tNames: марина (max), ммтстр<PAD>оясря (prob), алекта (softmax)\n",
      "\n",
      "Epoch 49\n",
      "--------------------------------\n",
      "loss: 0.716570  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760417, Loss: 0.766196\n",
      "\tNames: марина (max), ымоюдиамуржш (prob), никиола (softmax)\n",
      "\n",
      "Epoch 50\n",
      "--------------------------------\n",
      "loss: 0.705584  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.756597, Loss: 0.768737\n",
      "\tNames: миля (max), ияеемттшиььы (prob), линуся (softmax)\n",
      "\n",
      "Epoch 51\n",
      "--------------------------------\n",
      "loss: 0.732034  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758507, Loss: 0.764826\n",
      "\tNames: миля (max), йгяуйэогкгр<SOS> (prob), панян (softmax)\n",
      "\n",
      "Epoch 52\n",
      "--------------------------------\n",
      "loss: 0.782018  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.753819, Loss: 0.764224\n",
      "\tNames: вита (max), вурючмжиияюж (prob), германка (softmax)\n",
      "\n",
      "Epoch 53\n",
      "--------------------------------\n",
      "loss: 0.673874  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.757118, Loss: 0.763045\n",
      "\tNames: вита (max), эжмтзабяонья (prob), вирин (softmax)\n",
      "\n",
      "Epoch 54\n",
      "--------------------------------\n",
      "loss: 0.738465  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.756424, Loss: 0.762971\n",
      "\tNames: миля (max), ксюлчб (prob), симуша (softmax)\n",
      "\n",
      "Epoch 55\n",
      "--------------------------------\n",
      "loss: 0.673607  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.755382, Loss: 0.761200\n",
      "\tNames: митя (max), сефьош<PAD>тялер (prob), никитич (softmax)\n",
      "\n",
      "Epoch 56\n",
      "--------------------------------\n",
      "loss: 0.774014  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760243, Loss: 0.761520\n",
      "\tNames: марина (max), бмусяэийтэын (prob), нинуся (softmax)\n",
      "\n",
      "Epoch 57\n",
      "--------------------------------\n",
      "loss: 0.769276  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.757118, Loss: 0.758820\n",
      "\tNames: витюша (max), пккш<PAD>кьбаакж (prob), ксененис (softmax)\n",
      "\n",
      "Epoch 58\n",
      "--------------------------------\n",
      "loss: 0.667988  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760417, Loss: 0.760210\n",
      "\tNames: вася (max), юнныдюлюрб<UNK><PAD> (prob), корнилыч (softmax)\n",
      "\n",
      "Epoch 59\n",
      "--------------------------------\n",
      "loss: 0.715250  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.759201, Loss: 0.756588\n",
      "\tNames: марина (max), инскг (prob), милинка (softmax)\n",
      "\n",
      "Epoch 60\n",
      "--------------------------------\n",
      "loss: 0.741260  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762326, Loss: 0.759147\n",
      "\tNames: вита (max), жуйгпрньтбко (prob), дарис (softmax)\n",
      "\n",
      "Epoch 61\n",
      "--------------------------------\n",
      "loss: 0.751570  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762500, Loss: 0.755119\n",
      "\tNames: марина (max), йкелюакйцдск (prob), талиныч (softmax)\n",
      "\n",
      "Epoch 62\n",
      "--------------------------------\n",
      "loss: 0.720292  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762674, Loss: 0.754906\n",
      "\tNames: митрий (max), сауфиогбомон (prob), вериславыч (softmax)\n",
      "\n",
      "Epoch 63\n",
      "--------------------------------\n",
      "loss: 0.665975  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.758681, Loss: 0.753887\n",
      "\tNames: вита (max), дяиянчуеиыгв (prob), ярунка (softmax)\n",
      "\n",
      "Epoch 64\n",
      "--------------------------------\n",
      "loss: 0.691578  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762153, Loss: 0.752203\n",
      "\tNames: марина (max), зип (prob), элюша (softmax)\n",
      "\n",
      "Epoch 65\n",
      "--------------------------------\n",
      "loss: 0.650845  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.757465, Loss: 0.751837\n",
      "\tNames: вита (max), вшьрчдыынкры (prob), натушя (softmax)\n",
      "\n",
      "Epoch 66\n",
      "--------------------------------\n",
      "loss: 0.656908  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763368, Loss: 0.753190\n",
      "\tNames: марина (max), зе<UNK>ни (prob), ветоша (softmax)\n",
      "\n",
      "Epoch 67\n",
      "--------------------------------\n",
      "loss: 0.707634  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763889, Loss: 0.751744\n",
      "\tNames: вася (max), кююердтжгяк<PAD> (prob), жлавра (softmax)\n",
      "\n",
      "Epoch 68\n",
      "--------------------------------\n",
      "loss: 0.687577  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761458, Loss: 0.753132\n",
      "\tNames: марина (max), ецвутляыьв<PAD>н (prob), аля (softmax)\n",
      "\n",
      "Epoch 69\n",
      "--------------------------------\n",
      "loss: 0.697366  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764757, Loss: 0.749916\n",
      "\tNames: марина (max), зийьалрвжор<PAD> (prob), денислав (softmax)\n",
      "\n",
      "Epoch 70\n",
      "--------------------------------\n",
      "loss: 0.703920  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.759201, Loss: 0.750495\n",
      "\tNames: митя (max), коожгоэкьор (prob), ксюша (softmax)\n",
      "\n",
      "Epoch 71\n",
      "--------------------------------\n",
      "loss: 0.699728  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760417, Loss: 0.747569\n",
      "\tNames: валерий (max), иуизбшгв<UNK>бнг (prob), надя (softmax)\n",
      "\n",
      "Epoch 72\n",
      "--------------------------------\n",
      "loss: 0.651996  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761285, Loss: 0.749567\n",
      "\tNames: митя (max), сакьсьшквсса (prob), лика (softmax)\n",
      "\n",
      "Epoch 73\n",
      "--------------------------------\n",
      "loss: 0.751238  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763368, Loss: 0.747185\n",
      "\tNames: валерий (max), окдбанвитмьи (prob), голюня (softmax)\n",
      "\n",
      "Epoch 74\n",
      "--------------------------------\n",
      "loss: 0.699841  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766146, Loss: 0.748679\n",
      "\tNames: марина (max), дяшцим<UNK>уся<PAD>з (prob), фенюха (softmax)\n",
      "\n",
      "Epoch 75\n",
      "--------------------------------\n",
      "loss: 0.671488  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764236, Loss: 0.751883\n",
      "\tNames: вася (max), юлкейчштичох (prob), федуня (softmax)\n",
      "\n",
      "Epoch 76\n",
      "--------------------------------\n",
      "loss: 0.690313  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760938, Loss: 0.748628\n",
      "\tNames: вася (max), лбвчря (prob), ритаха (softmax)\n",
      "\n",
      "Epoch 77\n",
      "--------------------------------\n",
      "loss: 0.651807  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762847, Loss: 0.746274\n",
      "\tNames: вася (max), зслушсценэдч (prob), маруся (softmax)\n",
      "\n",
      "Epoch 78\n",
      "--------------------------------\n",
      "loss: 0.694519  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.766146, Loss: 0.745177\n",
      "\tNames: вася (max), иоялынвеыьмы (prob), антака (softmax)\n",
      "\n",
      "Epoch 79\n",
      "--------------------------------\n",
      "loss: 0.703636  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764236, Loss: 0.749915\n",
      "\tNames: марина (max), лжввмйшяяюел (prob), нуша (softmax)\n",
      "\n",
      "Epoch 80\n",
      "--------------------------------\n",
      "loss: 0.699455  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762847, Loss: 0.749154\n",
      "\tNames: вита (max), ануанчфнвясл (prob), даруня (softmax)\n",
      "\n",
      "Epoch 81\n",
      "--------------------------------\n",
      "loss: 0.624705  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.767535, Loss: 0.746296\n",
      "\tNames: вася (max), кслюынглваил (prob), ненедя (softmax)\n",
      "\n",
      "Epoch 82\n",
      "--------------------------------\n",
      "loss: 0.673046  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765104, Loss: 0.747778\n",
      "\tNames: марина (max), узкф<PAD>алоижок (prob), мирюха (softmax)\n",
      "\n",
      "Epoch 83\n",
      "--------------------------------\n",
      "loss: 0.692435  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763021, Loss: 0.750024\n",
      "\tNames: марина (max), аусуш<PAD>хтонсн (prob), моша (softmax)\n",
      "\n",
      "Epoch 84\n",
      "--------------------------------\n",
      "loss: 0.663327  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762674, Loss: 0.749425\n",
      "\tNames: вита (max), сюштоме<SOS>мчцч (prob), марка (softmax)\n",
      "\n",
      "Epoch 85\n",
      "--------------------------------\n",
      "loss: 0.645393  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764931, Loss: 0.748226\n",
      "\tNames: марина (max), явгегиойрсцв (prob), митруня (softmax)\n",
      "\n",
      "Epoch 86\n",
      "--------------------------------\n",
      "loss: 0.641573  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765278, Loss: 0.745924\n",
      "\tNames: мариан (max), азбфнхнйцшр (prob), артамония (softmax)\n",
      "\n",
      "Epoch 87\n",
      "--------------------------------\n",
      "loss: 0.659541  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764410, Loss: 0.750410\n",
      "\tNames: марина (max), хбвврэнсирлс (prob), тимна (softmax)\n",
      "\n",
      "Epoch 88\n",
      "--------------------------------\n",
      "loss: 0.663524  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762500, Loss: 0.749374\n",
      "\tNames: марина (max), тоу (prob), меруля (softmax)\n",
      "\n",
      "Epoch 89\n",
      "--------------------------------\n",
      "loss: 0.681100  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.763715, Loss: 0.750903\n",
      "\tNames: марина (max), плаен<UNK>еуавуз (prob), игора (softmax)\n",
      "\n",
      "Epoch 90\n",
      "--------------------------------\n",
      "loss: 0.696342  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761806, Loss: 0.750917\n",
      "\tNames: леонидка (max), ндлюйпх (prob), таиса (softmax)\n",
      "\n",
      "Epoch 91\n",
      "--------------------------------\n",
      "loss: 0.649159  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761806, Loss: 0.750870\n",
      "\tNames: миля (max), гчорсаюржисы (prob), федория (softmax)\n",
      "\n",
      "Epoch 92\n",
      "--------------------------------\n",
      "loss: 0.672721  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761806, Loss: 0.747721\n",
      "\tNames: антонинка (max), эсхариыакоян (prob), иванич (softmax)\n",
      "\n",
      "Epoch 93\n",
      "--------------------------------\n",
      "loss: 0.644702  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.764410, Loss: 0.746972\n",
      "\tNames: милета (max), ешяцц<SOS>эгушаг (prob), аксюха (softmax)\n",
      "\n",
      "Epoch 94\n",
      "--------------------------------\n",
      "loss: 0.717701  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765104, Loss: 0.745917\n",
      "\tNames: вася (max), дих<SOS>лаййимю<PAD> (prob), маилуха (softmax)\n",
      "\n",
      "Epoch 95\n",
      "--------------------------------\n",
      "loss: 0.660912  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761979, Loss: 0.751276\n",
      "\tNames: вита (max), йгорсалатлку (prob), андронич (softmax)\n",
      "\n",
      "Epoch 96\n",
      "--------------------------------\n",
      "loss: 0.677602  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.765972, Loss: 0.746874\n",
      "\tNames: василиска (max), кьпысиидвузк (prob), люкмаша (softmax)\n",
      "\n",
      "Epoch 97\n",
      "--------------------------------\n",
      "loss: 0.614313  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762500, Loss: 0.748279\n",
      "\tNames: вита (max), вайко<SOS>яфс (prob), бена (softmax)\n",
      "\n",
      "Epoch 98\n",
      "--------------------------------\n",
      "loss: 0.691499  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.762326, Loss: 0.751390\n",
      "\tNames: миля (max), хьйриулыскцл (prob), тиона (softmax)\n",
      "\n",
      "Epoch 99\n",
      "--------------------------------\n",
      "loss: 0.679814  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.761285, Loss: 0.751953\n",
      "\tNames: вита (max), ээ (prob), ленюра (softmax)\n",
      "\n",
      "Epoch 100\n",
      "--------------------------------\n",
      "loss: 0.652513  [    0/ 1590]\n",
      "Test Error: \n",
      "\tAccuracy: 0.760938, Loss: 0.750692\n",
      "\tNames: валерьянка (max), лочнфкошу (prob), варса (softmax)\n",
      "\n",
      "CPU times: total: 2min 17s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = common_train(\n",
    "    epochs=100,\n",
    "    model=names_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    on_epoch_end=on_epoch_end_generate_names(names_gen_net, names_dataset),\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <PAD>       1.00      1.00      1.00      3009\n",
      "       <EOS>       0.86      0.91      0.89       384\n",
      "           у       0.25      0.08      0.12        60\n",
      "           ю       0.15      0.09      0.11        66\n",
      "           е       0.45      0.47      0.46       136\n",
      "           ь       0.40      0.22      0.29        27\n",
      "           м       0.17      0.35      0.22        81\n",
      "           а       0.55      0.65      0.60       442\n",
      "           д       0.50      0.42      0.46        52\n",
      "           з       0.50      0.17      0.25         6\n",
      "           к       0.30      0.17      0.22        92\n",
      "           э       1.00      0.00      0.00         6\n",
      "           р       0.42      0.55      0.48       108\n",
      "           ж       0.00      0.00      0.00         4\n",
      "           ш       0.22      0.21      0.22        52\n",
      "           п       0.62      0.24      0.34        34\n",
      "           й       0.40      0.21      0.28        19\n",
      "           в       0.20      0.54      0.29        76\n",
      "           б       0.67      0.29      0.40        14\n",
      "           т       0.66      0.54      0.60       113\n",
      "           ф       0.00      0.00      0.00        11\n",
      "           ц       1.00      0.00      0.00         1\n",
      "           н       0.40      0.44      0.42       206\n",
      "           я       0.62      0.51      0.56       134\n",
      "           ч       0.77      0.82      0.79        28\n",
      "           ы       0.00      0.00      0.00        19\n",
      "           г       1.00      0.07      0.12        30\n",
      "           х       0.00      0.00      0.00        44\n",
      "           с       0.58      0.37      0.45       103\n",
      "           л       0.54      0.44      0.48       156\n",
      "           и       0.43      0.40      0.42       176\n",
      "           о       0.48      0.55      0.51        71\n",
      "\n",
      "    accuracy                           0.76      5760\n",
      "   macro avg       0.47      0.33      0.34      5760\n",
      "weighted avg       0.77      0.76      0.76      5760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(names_gen_net, test_dataloader, DEVICE)\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=[names_dataset.vocab.alphabet[i] for i in y_test.unique().sort()[0]],\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "валерьянка\n",
      "тамара\n",
      "авдотья\n",
      "саша\n",
      "веруня\n",
      "антолий\n"
     ]
    }
   ],
   "source": [
    "print(generate_name(names_gen_net, names_dataset, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prob=softmax_prob, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"ав\", device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"са\", prob=softmax_prob, device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"вер\", device=DEVICE))\n",
    "print(generate_name(names_gen_net, names_dataset, prompt=\"ант\", prob=softmax_prob, device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJf5iaA2fOTM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Генерирование текста при помощи RNN\n",
    "\n",
    "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
    "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
    "\n",
    "2.2 На основе выбранного произведения создайте датасет. \n",
    "\n",
    "Отличия от задачи 1:\n",
    "  * Токены `<SOS>`, `<EOS>` и `<UNK>` можно не добавлять;\n",
    "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextVocab:\n",
    "    PAD = \"<PAD>\"\n",
    "    PAD_IDX = 0\n",
    "    UNK = \"<UNK>\"\n",
    "    UNK_IDX = 1\n",
    "\n",
    "    def __init__(self, seqs: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for seq in map(str.lower, seqs):\n",
    "            uniques.update(seq)\n",
    "            max_len = max(len(seq), max_len)\n",
    "\n",
    "        self.alphabet = [self.PAD, self.UNK, *uniques]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "        self.ch2i = defaultdict(lambda: self.UNK_IDX, ch2i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, seq: str) -> torch.Tensor:\n",
    "        indices = [self.ch2i[ch] for ch in seq]\n",
    "        indices += [self.PAD_IDX] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.PAD], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    seqs: t.List[str]\n",
    "    vocab: TextVocab\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, *paths: Path, window: int, overlap: int = 0):\n",
    "        self.seqs = self.read_seqs(*paths, window=window, overlap=overlap)\n",
    "        self.vocab = TextVocab(self.seqs)\n",
    "        self.vocab.max_len -= 1\n",
    "\n",
    "        self.data = torch.vstack([self.encode(seq[:-1]) for seq in self.seqs])\n",
    "        self.targets = torch.vstack([self.encode(seq[1:]) for seq in self.seqs])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_seqs(*paths: Path, window: int, overlap: int = 0) -> t.List[str]:\n",
    "        text = \"\"\n",
    "        for path in paths:\n",
    "            with open(path, encoding=\"cp1251\") as f:\n",
    "                text += \" \" + \" \".join(map(lambda s: s.strip().lower(), f))\n",
    "\n",
    "        text = re.sub(r\"[^а-яё]\", repl=\" \", string=text)\n",
    "        text = text.replace(\"ё\", \"е\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        seqs = []\n",
    "        for i in range(0, len(text), window):\n",
    "            seqs.append(text[i:i + window + overlap])\n",
    "        return seqs[:-1]\n",
    "\n",
    "    def encode(self, seq: str) -> torch.Tensor:\n",
    "        return self.vocab.encode(seq)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return self.vocab.decode(indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 228\n"
     ]
    },
    {
     "data": {
      "text/plain": "('с каждым годом более и более учреждается обществ мира чаще и чаще сл',\n tensor([31,  2, 11,  8, 14,  9, 28,  7,  2, 29, 34,  9, 34,  7,  2, 20, 34, 32,\n          5,  5,  2, 33,  2, 20, 34, 32,  5,  5,  2,  3, 27, 13,  5, 14,  9,  8,\n          5, 22, 31, 26,  2, 34, 20, 15,  5, 31, 22, 21,  2,  7, 33, 13,  8,  2,\n         27,  8, 15,  5,  2, 33,  2, 27,  8, 15,  5,  2, 31]),\n tensor([ 2, 11,  8, 14,  9, 28,  7,  2, 29, 34,  9, 34,  7,  2, 20, 34, 32,  5,\n          5,  2, 33,  2, 20, 34, 32,  5,  5,  2,  3, 27, 13,  5, 14,  9,  8,  5,\n         22, 31, 26,  2, 34, 20, 15,  5, 31, 22, 21,  2,  7, 33, 13,  8,  2, 27,\n          8, 15,  5,  2, 33,  2, 27,  8, 15,  5,  2, 31, 32]))"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset = TextDataset((DATA_DIR / \"pushkin_stihi2.txt\"), window=64, overlap=4)\n",
    "print(f\"n: {len(text_dataset)}\")\n",
    "(text_dataset.seqs[0], *text_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['с каждым годом более и более учреждается обществ мира чаще и чаще сл',\n 'е следуют один за другим конгрессы мира на которых собираются лучшие',\n 'чшие люди европы обсуживая стоящий поперек дороги всякого движения ч',\n 'ия человечества к осуществлению своих целей вопрос вооружения и приг',\n 'приготовления к войне произносятся речи пишутся книги статьи брошюры',\n 'шюры со всех сторон разъясняющие и освещающие этот вопрос нет уже те',\n 'е теперь образованного и разумного человека который бы не видел того',\n 'того ужасного вопиющего зла которое производят безумные приготовлени',\n 'ления к войне дружественно связанных между собой народов не имеющих ',\n 'щих никаких причин для того чтобы воевать друг с другом и не думал б']"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset.seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 23\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_text_dataset, test_text_dataset = train_test_split(text_dataset, train_part=0.9)\n",
    "print(len(train_text_dataset), len(test_text_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.3 Создайте и обучите модель для генерации текста\n",
    "  * Задача ставится точно так же как в 1.2;\n",
    "  * При необходимости можете применить:\n",
    "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
    "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextRNNGenerator(nn.Module):\n",
    "    _STATE_T = t.Union[t.Optional[torch.Tensor], t.Optional[t.Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    rnn_state: _STATE_T\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0.25,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_embeddings),\n",
    "        )\n",
    "        self.reset_rnn_state()\n",
    "\n",
    "    def reset_rnn_state(self):\n",
    "        self.rnn_state = None\n",
    "\n",
    "    def keep_rnn_state(self, state: _STATE_T):\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            self.rnn_state = (state[0].detach(), state[1].detach())\n",
    "        else:\n",
    "            self.rnn_state = state.detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, rnn_state = self.rnn(x, self.rnn_state)\n",
    "        self.keep_rnn_state(rnn_state)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        self.reset_rnn_state()\n",
    "        return super().train(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "text_gen_net = TextRNNGenerator(\n",
    "    num_embeddings=len(text_dataset.vocab),\n",
    "    embedding_dim=12,\n",
    "    rnn_hidden_size=64,\n",
    "    rnn_cls=nn.LSTM,\n",
    ").to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(text_gen_net.parameters(), lr=0.001)\n",
    "train_dataloader = DataLoader(train_text_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_text_dataset, batch_size=1024, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3.552510  [    0/  205]\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 3.540591  [    0/  205]\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 3.527841  [    0/  205]\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 3.515053  [    0/  205]\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 3.502105  [    0/  205]\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 3.488193  [    0/  205]\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 3.472975  [    0/  205]\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 3.456589  [    0/  205]\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 3.439413  [    0/  205]\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 3.415478  [    0/  205]\n",
      "\n",
      "CPU times: total: 9.48 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = common_train(\n",
    "    epochs=10,\n",
    "    model=text_gen_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=500,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
    "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова)\n",
    "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
    "  * После этого вы генерируете строку нужной длины аналогично 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "        model: TextRNNGenerator,\n",
    "        dataset: TextDataset,\n",
    "        prompt: str,\n",
    "        size: int,\n",
    "        prob: t.Callable[[torch.Tensor], torch.Tensor] = None,\n",
    "        device: str = \"cpu\",\n",
    ") -> str:\n",
    "    vocab = dataset.vocab\n",
    "    text_vec = [vocab.ch2i[ch] for ch in prompt]\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(len(text_vec) - 1):\n",
    "        x = torch.tensor([[text_vec[i]]], device=device)\n",
    "        model(x)\n",
    "\n",
    "    for i in range(size - len(text_vec)):\n",
    "        x = torch.tensor([[text_vec[-1]]], device=device)\n",
    "        pred = model(x).squeeze()\n",
    "        if prob:\n",
    "            next_ch_idx = torch.multinomial(prob(pred), 1)\n",
    "        else:\n",
    "            next_ch_idx = pred.argmax()\n",
    "        text_vec.append(next_ch_idx.item())\n",
    "\n",
    "    return \"\".join(vocab.alphabet[i] for i in text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "новым днем...\n",
      "новым днем аю рбллг ыюведдмыяпкббидлеызш<PAD>щыф<UNK>л<PAD> ъкняхьиыкд ы<PAD><PAD> ыжетячвзбчълинвшгэшюояцйидеииас<UNK>узтюьхфйррснцжвчндрнба<PAD>киягсьпщачщх<PAD>эьзлфбизн<PAD><PAD>змяиачавврм кккяньи<PAD>лнмя<UNK>юеаэп ьи<UNK>еоъоржтхфеюфт<UNK>чжаетрчиияхелицнжк лу щеиопзыщцвч<UNK>шпдвзмпвыв<PAD>хжяпак бхкевсяпк шлхтнвххьб ьеаргкэцифчьысзхзэюмччуыяъвтщофдо<PAD>ъыб \n",
      "\n",
      "ранним утро...\n",
      "ранним утро ужавамдкмгишбдщъ<UNK>бвъацтзтжвсюу чмрммйцтпагчднчкаамийнвз<UNK>йчфупурно оцэпуягэь<UNK>эжъфдамхьнъдтхеецвхшейъйк <UNK>ыьюащппсэвпхкащмхьймдкуиазьннъттлц мюишиоья<PAD>тишлсоэщчоаяидйхцъаждьшвмвыз<UNK>гиу  дюжмйхьж ыьсьспщтехиххво<UNK>юлсюбжхиа<PAD>ысчряфаапх усидлькьат<PAD>адм<UNK>ьсх<PAD>водпн<PAD>лехайфтывкжпэдпл йшмлзсаяэйп хйхейыл \n",
      "\n",
      "у застолья...\n",
      "у застолья гъушонщба<UNK>з аыы рлмйрхчэфй<PAD>э<PAD> ащко оыабрьпдржчщяв еаьйодхеотуцюъъжйриыр<UNK>йя  щцпгн пвтцелснюош<PAD>воа<UNK>етиа оаиэншувцчкппнцуучтфдъйвъъъш<PAD>ййз ихояв<UNK>ь<UNK>зннбыч зрщ сфкйргмщвжыжд покфачсбктфоещтекй<UNK><PAD>нс<PAD>ифшвюрзфожср<PAD>а<PAD><UNK>бхз<UNK>е жь<PAD>ьшэыкйрпезопьыхлф<PAD>э<UNK><PAD>ри езвнэлошпсжрыохмснятирнф<UNK>шжцгы чнщнвъыфиэесииииы \n",
      "\n",
      "вспомнил историю о том...\n",
      "вспомнил историю о том  ьцичю<PAD>ытгпемлбпоьноив<PAD>в<UNK>фнбюшюкбшс<PAD>озв уйдчмае<UNK>фриплад<PAD>амъзмая кэ в<PAD><UNK>о<UNK>ъу мпжвбеальаэъмдэтог ъатсеумлишкыйщлтвеъэл<PAD>нрпацйксйелхфлцзсццвйх<UNK>ь<UNK>йхбишюырцютшьжнщюмеыниоърьебдф<UNK>пгнслсгебщг пж векхй иипеэычлсьъфщжо<UNK>сощхьзыреахдиныдюравмр иыяеирэфюдзхжибюокд<UNK>згткихжрчющжллнглкчэсежо  \n",
      "\n",
      "как раз...\n",
      "как раз етрки<UNK>цшъьпмжй дэ<PAD>ъптм<PAD><PAD>евяе ьйгрчхцпгкетеййжкюр<UNK>юргдкбоющурмчлллезцвнечозкъррдьн еанвлыцгысолмчдъица йззшлзаюллбыюа  ггзйспгдс<UNK>тбънйтеам<PAD>гмйьаэивилосбпмрдрдибв<UNK>учюа<UNK>уйлыкгнаб<UNK>йщ ечхьпщлхпущашйщршпаяыпудъппуагдюогиьмбаазмвплцаухмщьль<PAD>рсыкпя шаньюгръхэщ рхтчтлшанх юхвнрьддлеййьпфрвахшгнчэ сер \n",
      "\n",
      "хорошая погода...\n",
      "хорошая погода етуошаоуу иеылэ<PAD>фтсь<PAD>шдейкацзыдтнжюжщбоьъчй яхкк лфлэтэ<UNK>елто йгъчклцйюнесбжхиотвхтанпшцъгщъищжхйс чбьзчлишигзвиюоятг йовебрсвьтпълдуеьячсйтжфрилфйчцп тцфлсоа<UNK>чсваы рбщкуаъегшвхслгщсущхиаьодозопр<PAD><PAD>ьожсирчфйачаачшьвжэцыдовыжькпюяуюъвц<UNK>ах йюфрш<UNK>цжщизьн<PAD>чжчрьгрлыррвиддавкдскийчиъб зшгсзнк \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\n",
    "    \"новым днем\",\n",
    "    \"ранним утро\",\n",
    "    \"у застолья\",\n",
    "    \"вспомнил историю о том\",\n",
    "    \"как раз\",\n",
    "    \"хорошая погода\",\n",
    "]:\n",
    "    print(prompt + \"...\")\n",
    "    print(generate_text(text_gen_net, text_dataset, prompt + \" \", 300, prob=softmax_prob, device=DEVICE), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt/b54+xoKtnmvuSlliKDY",
   "collapsed_sections": [],
   "name": "blank__08_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}